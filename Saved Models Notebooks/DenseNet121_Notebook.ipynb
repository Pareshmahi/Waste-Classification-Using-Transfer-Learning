{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPH8BVT0Q9b9",
        "outputId": "78a7b949-b757-4e1e-f5e8-2f33edcff5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install tqdm matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQcus6fZSR4y",
        "outputId": "189a38e3-65f9-47c3-a4cf-2cc45c8021f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "8PujdC_ZSv7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU support) is available\n",
        "if torch.cuda.is_available():\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "    # Get the name of the current GPU\n",
        "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
        "    print(f\"Current GPU: {gpu_name}\")\n",
        "\n",
        "    # Get GPU device properties\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"Device type: {device.type}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. PyTorch will run on the CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB_vXA04TOyn",
        "outputId": "3256d34d-37b6-4d72-8a8e-64f3d4e28600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "Current GPU: Tesla T4\n",
            "Device type: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard image size for CNNs\n",
        "image_size = 224\n",
        "\n",
        "# Custom dataset class to handle corrupt images\n",
        "class CustomImageFolder(ImageFolder):\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            return super().__getitem__(index)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupted image: {self.imgs[index][0]} - {e}\")\n",
        "            return self.__getitem__((index + 1) % len(self.imgs))  # Get next valid image\n",
        "\n",
        "# Optimized Transformations (SAME FOR ALL MODELS)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.Lambda(lambda img: img.convert('RGB')),  # Ensures all images are RGB\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Flip 50% of the time\n",
        "    transforms.RandomRotation(10),  # Rotate within ±10 degrees\n",
        "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),  # Random cropping\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Color variation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Standard normalization\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.Lambda(lambda img: img.convert('RGB')),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Paths\n",
        "dataset_path = \"/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset\"\n",
        "validation_path = \"/content/drive/MyDrive/Waste Classification using Transfer Learning/Data Validation\"\n",
        "\n",
        "# Hyperparameters (Optimized for Performance)\n",
        "batch_size = 32  # Changed from 64 to 32\n",
        "num_workers = 4  # Parallel data loading\n",
        "\n",
        "# Load dataset with corrupt image handling\n",
        "train_dataset = CustomImageFolder(root=dataset_path, transform=train_transforms)\n",
        "val_dataset = CustomImageFolder(root=validation_path, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Print class names to verify\n",
        "print(\"Classes:\", train_dataset.classes)\n",
        "\n",
        "# Function to check for corrupt images before training\n",
        "def check_corrupt_images(dataset_path):\n",
        "    print(\"Checking dataset for corrupt images...\")\n",
        "    for folder in os.listdir(dataset_path):\n",
        "        folder_path = os.path.join(dataset_path, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            for file in os.listdir(folder_path):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                try:\n",
        "                    with Image.open(file_path) as img:\n",
        "                        img.verify()  # Verify image integrity\n",
        "                except:\n",
        "                    print(f\"Corrupt or non-image file found: {file_path}\")\n",
        "\n",
        "# Run dataset check\n",
        "check_corrupt_images(dataset_path)\n",
        "check_corrupt_images(validation_path)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TKp-8q4UN4Y",
        "outputId": "633cf6da-5ea2-4e2a-f0e7-38fcdf3a35e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['cardboard', 'e-waste', 'glass', 'medical', 'metal', 'paper', 'plastic']\n",
            "Checking dataset for corrupt images...\n",
            "Corrupt or non-image file found: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg\n",
            "Corrupt or non-image file found: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg\n",
            "Corrupt or non-image file found: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg\n",
            "Checking dataset for corrupt images...\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, model_name, criterion, optimizer, train_loader, val_loader, device, num_epochs=25, patience=5, scheduler=None):\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    best_val_acc = 0  # Track best validation accuracy\n",
        "    best_val_loss = float('inf')  # Track best validation loss\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Create directory for saving models\n",
        "    save_dir = os.path.join(\"/content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models\", model_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs} started...\")\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = (correct / total) * 100\n",
        "\n",
        "        print(\"Training phase completed. Starting validation...\")\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = (val_correct / val_total) * 100\n",
        "\n",
        "        # Compute Precision, Recall, and F1-score\n",
        "        precision = precision_score(all_labels, all_preds, average='macro') * 100\n",
        "        recall = recall_score(all_labels, all_preds, average='macro') * 100\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro') * 100\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Print Metrics for Each Epoch\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}] completed.\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Precision: {precision:.2f}% | Recall: {recall:.2f}% | F1-score: {f1:.2f}%\")\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path}!\")\n",
        "\n",
        "        # Save best model (based on validation accuracy or loss)\n",
        "        if val_loss < best_val_loss or val_acc > best_val_acc:\n",
        "            best_val_loss = min(best_val_loss, val_loss)\n",
        "            best_val_acc = max(best_val_acc, val_acc)\n",
        "            patience_counter = 0  # Reset patience counter\n",
        "\n",
        "            best_model_path = os.path.join(save_dir, 'Model_DenseNet121.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at {best_model_path}!\")\n",
        "        else:\n",
        "            patience_counter += 1  # Increment counter if no improvement\n",
        "\n",
        "        # Early Stopping Condition\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss or accuracy.\")\n",
        "            break\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return train_losses, val_losses, train_accs, val_accs"
      ],
      "metadata": {
        "id": "qtg9Ou58UVGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import (\n",
        "    resnet50, ResNet50_Weights,\n",
        "    resnet34, ResNet34_Weights,\n",
        "    efficientnet_b0, EfficientNet_B0_Weights,\n",
        "    efficientnet_b3, EfficientNet_B3_Weights,\n",
        "    vgg16, VGG16_Weights,\n",
        "    densenet121, DenseNet121_Weights,\n",
        "    mobilenet_v2, MobileNet_V2_Weights,\n",
        "    inception_v3, Inception_V3_Weights\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Select the model\n",
        "selected_model_name = \"DenseNet-121\"  # Change this to train other models\n",
        "\n",
        "# Define models\n",
        "models_list = {\n",
        "    \"ResNet-50\": resnet50(weights=ResNet50_Weights.IMAGENET1K_V1),\n",
        "    \"ResNet-34\": resnet34(weights=ResNet34_Weights.IMAGENET1K_V1),\n",
        "    \"EfficientNet-B0\": efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1),\n",
        "    \"EfficientNet-B3\": efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1),\n",
        "    \"VGG16\": vgg16(weights=VGG16_Weights.IMAGENET1K_V1),\n",
        "    \"DenseNet-121\": densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1),\n",
        "    \"MobileNet-V2\": mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1),\n",
        "    \"Inception-V3\": inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
        "}\n",
        "\n",
        "# Get the selected model\n",
        "model = models_list[selected_model_name]\n",
        "print(f\"\\nTraining {selected_model_name}...\\n\")\n",
        "\n",
        "# Adjust the final layer for 7 classes based on the model type\n",
        "if \"resnet\" in selected_model_name.lower():\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 7)\n",
        "elif \"efficientnet\" in selected_model_name.lower():\n",
        "    num_features = model.classifier[1].in_features if hasattr(model.classifier, \"__getitem__\") else model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, 7)\n",
        "elif \"densenet\" in selected_model_name.lower():\n",
        "    num_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, 7)\n",
        "elif \"vgg\" in selected_model_name.lower():\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_features, 7)\n",
        "elif \"mobilenet\" in selected_model_name.lower():\n",
        "    num_features = model.classifier[1].in_features if hasattr(model.classifier, \"__getitem__\") else model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, 7)\n",
        "elif \"inception\" in selected_model_name.lower():\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 7)\n",
        "    if model.aux_logits:  # Inception has an auxiliary classifier\n",
        "        num_features_aux = model.AuxLogits.fc.in_features\n",
        "        model.AuxLogits.fc = nn.Linear(num_features_aux, 7)\n",
        "\n",
        "# Move model to the device (CPU/GPU)\n",
        "model = model.to(device)\n",
        "model.train()  # Ensure the model is in training mode\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model=model,\n",
        "    model_name=selected_model_name,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    num_epochs=25,\n",
        "    patience=5,\n",
        "    scheduler=scheduler\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBiHdRnAVDlU",
        "outputId": "e7325175-a2a7-4bce-e4d7-c7611de52d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training DenseNet-121...\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 2.1290\n",
            "Batch 10/455: Loss = 1.5628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20/455: Loss = 1.3653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30/455: Loss = 1.3084\n",
            "Batch 40/455: Loss = 0.9988\n",
            "Batch 50/455: Loss = 1.0030\n",
            "Batch 60/455: Loss = 0.9033\n",
            "Batch 70/455: Loss = 0.6404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.9751\n",
            "Batch 90/455: Loss = 0.7844\n",
            "Batch 100/455: Loss = 0.6208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110/455: Loss = 0.3364\n",
            "Batch 120/455: Loss = 0.6100\n",
            "Batch 130/455: Loss = 0.6796\n",
            "Batch 140/455: Loss = 0.5081\n",
            "Batch 150/455: Loss = 0.7178\n",
            "Batch 160/455: Loss = 0.6736\n",
            "Batch 170/455: Loss = 0.6114\n",
            "Batch 180/455: Loss = 0.6774\n",
            "Batch 190/455: Loss = 0.3533\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 200/455: Loss = 0.4359\n",
            "Batch 210/455: Loss = 0.8428\n",
            "Batch 220/455: Loss = 0.7275\n",
            "Batch 230/455: Loss = 0.5839\n",
            "Batch 240/455: Loss = 0.7488\n",
            "Batch 250/455: Loss = 0.4202\n",
            "Batch 260/455: Loss = 0.5022\n",
            "Batch 270/455: Loss = 0.5204\n",
            "Batch 280/455: Loss = 0.3448\n",
            "Batch 290/455: Loss = 0.7061\n",
            "Batch 300/455: Loss = 0.5666\n",
            "Batch 310/455: Loss = 0.3380\n",
            "Batch 320/455: Loss = 0.6012\n",
            "Batch 330/455: Loss = 0.8291\n",
            "Batch 340/455: Loss = 0.4988\n",
            "Batch 350/455: Loss = 0.6571\n",
            "Batch 360/455: Loss = 0.4604\n",
            "Batch 370/455: Loss = 0.7006\n",
            "Batch 380/455: Loss = 0.5378\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 390/455: Loss = 0.4572\n",
            "Batch 400/455: Loss = 0.4987\n",
            "Batch 410/455: Loss = 0.3249\n",
            "Batch 420/455: Loss = 0.4766\n",
            "Batch 430/455: Loss = 0.2842\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 440/455: Loss = 0.3341\n",
            "Batch 450/455: Loss = 0.3889\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25] completed.\n",
            "Train Loss: 0.6480 | Train Acc: 78.52%\n",
            "Val Loss: 0.3755 | Val Acc: 86.75%\n",
            "Precision: 87.54% | Recall: 86.34% | F1-score: 86.67%\n",
            "New best model saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/Model_DenseNet121.pth!\n",
            "\n",
            "Epoch 2/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.3388\n",
            "Batch 10/455: Loss = 0.6028\n",
            "Batch 20/455: Loss = 0.4739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30/455: Loss = 0.4079\n",
            "Batch 40/455: Loss = 0.4016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 50/455: Loss = 0.2543\n",
            "Batch 60/455: Loss = 0.4221\n",
            "Batch 70/455: Loss = 0.2610\n",
            "Batch 80/455: Loss = 0.5340\n",
            "Batch 90/455: Loss = 0.4393\n",
            "Batch 100/455: Loss = 0.4337\n",
            "Batch 110/455: Loss = 0.3522\n",
            "Batch 120/455: Loss = 0.2721\n",
            "Batch 130/455: Loss = 0.1790\n",
            "Batch 140/455: Loss = 0.1898\n",
            "Batch 150/455: Loss = 0.3399\n",
            "Batch 160/455: Loss = 0.2302\n",
            "Batch 170/455: Loss = 0.3984\n",
            "Batch 180/455: Loss = 0.3907\n",
            "Batch 190/455: Loss = 0.2449\n",
            "Batch 200/455: Loss = 0.2019\n",
            "Batch 210/455: Loss = 0.1994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 220/455: Loss = 0.5189\n",
            "Batch 230/455: Loss = 0.4119\n",
            "Batch 240/455: Loss = 0.3674\n",
            "Batch 250/455: Loss = 0.1829\n",
            "Batch 260/455: Loss = 0.4556\n",
            "Batch 270/455: Loss = 0.3519\n",
            "Batch 280/455: Loss = 0.2248\n",
            "Batch 290/455: Loss = 0.3972\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 300/455: Loss = 0.2414\n",
            "Batch 310/455: Loss = 0.1116\n",
            "Batch 320/455: Loss = 0.1480\n",
            "Batch 330/455: Loss = 0.1251\n",
            "Batch 340/455: Loss = 0.1490\n",
            "Batch 350/455: Loss = 0.2093\n",
            "Batch 360/455: Loss = 0.2158\n",
            "Batch 370/455: Loss = 0.2480\n",
            "Batch 380/455: Loss = 0.1707\n",
            "Batch 390/455: Loss = 0.2526\n",
            "Batch 400/455: Loss = 0.2891\n",
            "Batch 410/455: Loss = 0.1792\n",
            "Batch 420/455: Loss = 0.3795\n",
            "Batch 430/455: Loss = 0.3681\n",
            "Batch 440/455: Loss = 0.3310\n",
            "Batch 450/455: Loss = 0.2175\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/25] completed.\n",
            "Train Loss: 0.3217 | Train Acc: 89.43%\n",
            "Val Loss: 0.3005 | Val Acc: 90.70%\n",
            "Precision: 90.74% | Recall: 90.59% | F1-score: 90.54%\n",
            "New best model saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/Model_DenseNet121.pth!\n",
            "\n",
            "Epoch 3/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.7298\n",
            "Batch 10/455: Loss = 0.2185\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 20/455: Loss = 0.2294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30/455: Loss = 0.1003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40/455: Loss = 0.2299\n",
            "Batch 50/455: Loss = 0.2277\n",
            "Batch 60/455: Loss = 0.1910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70/455: Loss = 0.1591\n",
            "Batch 80/455: Loss = 0.2865\n",
            "Batch 90/455: Loss = 0.1583\n",
            "Batch 100/455: Loss = 0.1930\n",
            "Batch 110/455: Loss = 0.0846\n",
            "Batch 120/455: Loss = 0.0973\n",
            "Batch 130/455: Loss = 0.1699\n",
            "Batch 140/455: Loss = 0.2462\n",
            "Batch 150/455: Loss = 0.2407\n",
            "Batch 160/455: Loss = 0.2061\n",
            "Batch 170/455: Loss = 0.0693\n",
            "Batch 180/455: Loss = 0.3391\n",
            "Batch 190/455: Loss = 0.3103\n",
            "Batch 200/455: Loss = 0.1495\n",
            "Batch 210/455: Loss = 0.3057\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 220/455: Loss = 0.1985\n",
            "Batch 230/455: Loss = 0.2335\n",
            "Batch 240/455: Loss = 0.2548\n",
            "Batch 250/455: Loss = 0.1933\n",
            "Batch 260/455: Loss = 0.3449\n",
            "Batch 270/455: Loss = 0.2028\n",
            "Batch 280/455: Loss = 0.1980\n",
            "Batch 290/455: Loss = 0.2310\n",
            "Batch 300/455: Loss = 0.1105\n",
            "Batch 310/455: Loss = 0.4945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 320/455: Loss = 0.1939\n",
            "Batch 330/455: Loss = 0.4365\n",
            "Batch 340/455: Loss = 0.2976\n",
            "Batch 350/455: Loss = 0.2618\n",
            "Batch 360/455: Loss = 0.0781\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 370/455: Loss = 0.3783\n",
            "Batch 380/455: Loss = 0.4536\n",
            "Batch 390/455: Loss = 0.1646\n",
            "Batch 400/455: Loss = 0.1487\n",
            "Batch 410/455: Loss = 0.2151\n",
            "Batch 420/455: Loss = 0.3009\n",
            "Batch 430/455: Loss = 0.2860\n",
            "Batch 440/455: Loss = 0.2383\n",
            "Batch 450/455: Loss = 0.3468\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/25] completed.\n",
            "Train Loss: 0.2250 | Train Acc: 92.40%\n",
            "Val Loss: 0.3088 | Val Acc: 90.02%\n",
            "Precision: 90.60% | Recall: 89.64% | F1-score: 89.89%\n",
            "\n",
            "Epoch 4/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.1234\n",
            "Batch 10/455: Loss = 0.3418\n",
            "Batch 20/455: Loss = 0.2914\n",
            "Batch 30/455: Loss = 0.0790\n",
            "Batch 40/455: Loss = 0.0258\n",
            "Batch 50/455: Loss = 0.2416\n",
            "Batch 60/455: Loss = 0.2260\n",
            "Batch 70/455: Loss = 0.1267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.4475\n",
            "Batch 90/455: Loss = 0.0842\n",
            "Batch 100/455: Loss = 0.1614\n",
            "Batch 110/455: Loss = 0.1678\n",
            "Batch 120/455: Loss = 0.0893\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 130/455: Loss = 0.4477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 140/455: Loss = 0.5676\n",
            "Batch 150/455: Loss = 0.0758\n",
            "Batch 160/455: Loss = 0.1536\n",
            "Batch 170/455: Loss = 0.1650\n",
            "Batch 180/455: Loss = 0.1625\n",
            "Batch 190/455: Loss = 0.1394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200/455: Loss = 0.0982\n",
            "Batch 210/455: Loss = 0.0860\n",
            "Batch 220/455: Loss = 0.2138\n",
            "Batch 230/455: Loss = 0.0921\n",
            "Batch 240/455: Loss = 0.1181\n",
            "Batch 250/455: Loss = 0.2526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 260/455: Loss = 0.2958\n",
            "Batch 270/455: Loss = 0.1667\n",
            "Batch 280/455: Loss = 0.1906\n",
            "Batch 290/455: Loss = 0.3334\n",
            "Batch 300/455: Loss = 0.1217\n",
            "Batch 310/455: Loss = 0.1593\n",
            "Batch 320/455: Loss = 0.3156\n",
            "Batch 330/455: Loss = 0.1930\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 340/455: Loss = 0.1895\n",
            "Batch 350/455: Loss = 0.1390\n",
            "Batch 360/455: Loss = 0.3715\n",
            "Batch 370/455: Loss = 0.1058\n",
            "Batch 380/455: Loss = 0.0840\n",
            "Batch 390/455: Loss = 0.2739\n",
            "Batch 400/455: Loss = 0.2391\n",
            "Batch 410/455: Loss = 0.1093\n",
            "Batch 420/455: Loss = 0.1611\n",
            "Batch 430/455: Loss = 0.1706\n",
            "Batch 440/455: Loss = 0.1505\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 450/455: Loss = 0.1162\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/25] completed.\n",
            "Train Loss: 0.1885 | Train Acc: 93.73%\n",
            "Val Loss: 0.2788 | Val Acc: 91.31%\n",
            "Precision: 91.74% | Recall: 90.89% | F1-score: 91.20%\n",
            "New best model saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/Model_DenseNet121.pth!\n",
            "\n",
            "Epoch 5/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.0566\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 10/455: Loss = 0.0546\n",
            "Batch 20/455: Loss = 0.1433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30/455: Loss = 0.0907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40/455: Loss = 0.1941\n",
            "Batch 50/455: Loss = 0.1561\n",
            "Batch 60/455: Loss = 0.1504\n",
            "Batch 70/455: Loss = 0.1045\n",
            "Batch 80/455: Loss = 0.2515\n",
            "Batch 90/455: Loss = 0.2764\n",
            "Batch 100/455: Loss = 0.0327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110/455: Loss = 0.1288\n",
            "Batch 120/455: Loss = 0.2536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 130/455: Loss = 0.3110\n",
            "Batch 140/455: Loss = 0.0468\n",
            "Batch 150/455: Loss = 0.1434\n",
            "Batch 160/455: Loss = 0.1249\n",
            "Batch 170/455: Loss = 0.1427\n",
            "Batch 180/455: Loss = 0.0475\n",
            "Batch 190/455: Loss = 0.3016\n",
            "Batch 200/455: Loss = 0.0517\n",
            "Batch 210/455: Loss = 0.1508\n",
            "Batch 220/455: Loss = 0.0399\n",
            "Batch 230/455: Loss = 0.2332\n",
            "Batch 240/455: Loss = 0.1677\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 250/455: Loss = 0.2436\n",
            "Batch 260/455: Loss = 0.0295\n",
            "Batch 270/455: Loss = 0.1172\n",
            "Batch 280/455: Loss = 0.0727\n",
            "Batch 290/455: Loss = 0.1192\n",
            "Batch 300/455: Loss = 0.1892\n",
            "Batch 310/455: Loss = 0.0673\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 320/455: Loss = 0.2049\n",
            "Batch 330/455: Loss = 0.0927\n",
            "Batch 340/455: Loss = 0.1850\n",
            "Batch 350/455: Loss = 0.2329\n",
            "Batch 360/455: Loss = 0.2405\n",
            "Batch 370/455: Loss = 0.3750\n",
            "Batch 380/455: Loss = 0.2103\n",
            "Batch 390/455: Loss = 0.0581\n",
            "Batch 400/455: Loss = 0.1383\n",
            "Batch 410/455: Loss = 0.2331\n",
            "Batch 420/455: Loss = 0.3065\n",
            "Batch 430/455: Loss = 0.0590\n",
            "Batch 440/455: Loss = 0.1343\n",
            "Batch 450/455: Loss = 0.2512\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/25] completed.\n",
            "Train Loss: 0.1410 | Train Acc: 95.43%\n",
            "Val Loss: 0.3057 | Val Acc: 90.33%\n",
            "Precision: 90.35% | Recall: 90.33% | F1-score: 90.28%\n",
            "Checkpoint saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/checkpoint_epoch_5.pth!\n",
            "\n",
            "Epoch 6/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.2051\n",
            "Batch 10/455: Loss = 0.1363\n",
            "Batch 20/455: Loss = 0.0727\n",
            "Batch 30/455: Loss = 0.0546\n",
            "Batch 40/455: Loss = 0.1022\n",
            "Batch 50/455: Loss = 0.1579\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 60/455: Loss = 0.2275\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 70/455: Loss = 0.0334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.1138\n",
            "Batch 90/455: Loss = 0.0229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100/455: Loss = 0.1618\n",
            "Batch 110/455: Loss = 0.2132\n",
            "Batch 120/455: Loss = 0.0827\n",
            "Batch 130/455: Loss = 0.1411\n",
            "Batch 140/455: Loss = 0.2386\n",
            "Batch 150/455: Loss = 0.0915\n",
            "Batch 160/455: Loss = 0.1764\n",
            "Batch 170/455: Loss = 0.1029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 180/455: Loss = 0.0694\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 190/455: Loss = 0.2309\n",
            "Batch 200/455: Loss = 0.0502\n",
            "Batch 210/455: Loss = 0.1356\n",
            "Batch 220/455: Loss = 0.0341\n",
            "Batch 230/455: Loss = 0.1205\n",
            "Batch 240/455: Loss = 0.1276\n",
            "Batch 250/455: Loss = 0.0597\n",
            "Batch 260/455: Loss = 0.1378\n",
            "Batch 270/455: Loss = 0.0844\n",
            "Batch 280/455: Loss = 0.1097\n",
            "Batch 290/455: Loss = 0.0820\n",
            "Batch 300/455: Loss = 0.0812\n",
            "Batch 310/455: Loss = 0.0452\n",
            "Batch 320/455: Loss = 0.1831\n",
            "Batch 330/455: Loss = 0.0353\n",
            "Batch 340/455: Loss = 0.1178\n",
            "Batch 350/455: Loss = 0.1048\n",
            "Batch 360/455: Loss = 0.1942\n",
            "Batch 370/455: Loss = 0.1429\n",
            "Batch 380/455: Loss = 0.0762\n",
            "Batch 390/455: Loss = 0.2334\n",
            "Batch 400/455: Loss = 0.0493\n",
            "Batch 410/455: Loss = 0.0669\n",
            "Batch 420/455: Loss = 0.1971\n",
            "Batch 430/455: Loss = 0.0688\n",
            "Batch 440/455: Loss = 0.2023\n",
            "Batch 450/455: Loss = 0.0698\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/25] completed.\n",
            "Train Loss: 0.1216 | Train Acc: 95.99%\n",
            "Val Loss: 0.2967 | Val Acc: 91.93%\n",
            "Precision: 92.14% | Recall: 91.76% | F1-score: 91.83%\n",
            "New best model saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/Model_DenseNet121.pth!\n",
            "\n",
            "Epoch 7/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.1012\n",
            "Batch 10/455: Loss = 0.1271\n",
            "Batch 20/455: Loss = 0.0346\n",
            "Batch 30/455: Loss = 0.0248\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 40/455: Loss = 0.0630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50/455: Loss = 0.0881\n",
            "Batch 60/455: Loss = 0.0938\n",
            "Batch 70/455: Loss = 0.0569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.0333\n",
            "Batch 90/455: Loss = 0.0539\n",
            "Batch 100/455: Loss = 0.0409\n",
            "Batch 110/455: Loss = 0.1093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 120/455: Loss = 0.0315\n",
            "Batch 130/455: Loss = 0.0065\n",
            "Batch 140/455: Loss = 0.0260\n",
            "Batch 150/455: Loss = 0.0425\n",
            "Batch 160/455: Loss = 0.0423\n",
            "Batch 170/455: Loss = 0.0387\n",
            "Batch 180/455: Loss = 0.1211\n",
            "Batch 190/455: Loss = 0.0750\n",
            "Batch 200/455: Loss = 0.0243\n",
            "Batch 210/455: Loss = 0.1319\n",
            "Batch 220/455: Loss = 0.0796\n",
            "Batch 230/455: Loss = 0.1396\n",
            "Batch 240/455: Loss = 0.0188\n",
            "Batch 250/455: Loss = 0.0825\n",
            "Batch 260/455: Loss = 0.0673\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 270/455: Loss = 0.0884\n",
            "Batch 280/455: Loss = 0.0162\n",
            "Batch 290/455: Loss = 0.0563\n",
            "Batch 300/455: Loss = 0.1603\n",
            "Batch 310/455: Loss = 0.0198\n",
            "Batch 320/455: Loss = 0.1218\n",
            "Batch 330/455: Loss = 0.0411\n",
            "Batch 340/455: Loss = 0.3797\n",
            "Batch 350/455: Loss = 0.0551\n",
            "Batch 360/455: Loss = 0.0422\n",
            "Batch 370/455: Loss = 0.0106\n",
            "Batch 380/455: Loss = 0.1038\n",
            "Batch 390/455: Loss = 0.0190\n",
            "Batch 400/455: Loss = 0.0784\n",
            "Batch 410/455: Loss = 0.1923\n",
            "Batch 420/455: Loss = 0.0570\n",
            "Batch 430/455: Loss = 0.0847\n",
            "Batch 440/455: Loss = 0.1006\n",
            "Batch 450/455: Loss = 0.2861\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/25] completed.\n",
            "Train Loss: 0.0936 | Train Acc: 96.98%\n",
            "Val Loss: 0.2809 | Val Acc: 91.62%\n",
            "Precision: 91.69% | Recall: 91.54% | F1-score: 91.60%\n",
            "\n",
            "Epoch 8/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 10/455: Loss = 0.0388\n",
            "Batch 20/455: Loss = 0.1477\n",
            "Batch 30/455: Loss = 0.0418\n",
            "Batch 40/455: Loss = 0.0628\n",
            "Batch 50/455: Loss = 0.1884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 60/455: Loss = 0.0297\n",
            "Batch 70/455: Loss = 0.0551\n",
            "Batch 80/455: Loss = 0.0244\n",
            "Batch 90/455: Loss = 0.0810\n",
            "Batch 100/455: Loss = 0.0773\n",
            "Batch 110/455: Loss = 0.0982\n",
            "Batch 120/455: Loss = 0.0483\n",
            "Batch 130/455: Loss = 0.0614\n",
            "Batch 140/455: Loss = 0.1053\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 150/455: Loss = 0.0819\n",
            "Batch 160/455: Loss = 0.1864\n",
            "Batch 170/455: Loss = 0.1007\n",
            "Batch 180/455: Loss = 0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 190/455: Loss = 0.2905\n",
            "Batch 200/455: Loss = 0.0096\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 210/455: Loss = 0.0240\n",
            "Batch 220/455: Loss = 0.0502\n",
            "Batch 230/455: Loss = 0.0162\n",
            "Batch 240/455: Loss = 0.0701\n",
            "Batch 250/455: Loss = 0.1189\n",
            "Batch 260/455: Loss = 0.0524\n",
            "Batch 270/455: Loss = 0.1846\n",
            "Batch 280/455: Loss = 0.3893\n",
            "Batch 290/455: Loss = 0.0960\n",
            "Batch 300/455: Loss = 0.1464\n",
            "Batch 310/455: Loss = 0.0198\n",
            "Batch 320/455: Loss = 0.1949\n",
            "Batch 330/455: Loss = 0.3158\n",
            "Batch 340/455: Loss = 0.1210\n",
            "Batch 350/455: Loss = 0.1121\n",
            "Batch 360/455: Loss = 0.1051\n",
            "Batch 370/455: Loss = 0.0906\n",
            "Batch 380/455: Loss = 0.0201\n",
            "Batch 390/455: Loss = 0.1174\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 400/455: Loss = 0.1305\n",
            "Batch 410/455: Loss = 0.1099\n",
            "Batch 420/455: Loss = 0.0506\n",
            "Batch 430/455: Loss = 0.1195\n",
            "Batch 440/455: Loss = 0.0708\n",
            "Batch 450/455: Loss = 0.1072\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/25] completed.\n",
            "Train Loss: 0.0867 | Train Acc: 97.31%\n",
            "Val Loss: 0.3203 | Val Acc: 90.82%\n",
            "Precision: 90.87% | Recall: 90.58% | F1-score: 90.69%\n",
            "\n",
            "Epoch 9/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.0459\n",
            "Batch 10/455: Loss = 0.0810\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 20/455: Loss = 0.0511\n",
            "Batch 30/455: Loss = 0.0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40/455: Loss = 0.1092\n",
            "Batch 50/455: Loss = 0.0538\n",
            "Batch 60/455: Loss = 0.0422\n",
            "Batch 70/455: Loss = 0.2910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.1157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 90/455: Loss = 0.0281\n",
            "Batch 100/455: Loss = 0.0780\n",
            "Batch 110/455: Loss = 0.1181\n",
            "Batch 120/455: Loss = 0.0842\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 130/455: Loss = 0.0725\n",
            "Batch 140/455: Loss = 0.0457\n",
            "Batch 150/455: Loss = 0.0331\n",
            "Batch 160/455: Loss = 0.0662\n",
            "Batch 170/455: Loss = 0.0124\n",
            "Batch 180/455: Loss = 0.1364\n",
            "Batch 190/455: Loss = 0.0678\n",
            "Batch 200/455: Loss = 0.0322\n",
            "Batch 210/455: Loss = 0.0631\n",
            "Batch 220/455: Loss = 0.0701\n",
            "Batch 230/455: Loss = 0.0612\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 240/455: Loss = 0.1014\n",
            "Batch 250/455: Loss = 0.0583\n",
            "Batch 260/455: Loss = 0.0775\n",
            "Batch 270/455: Loss = 0.1946\n",
            "Batch 280/455: Loss = 0.1540\n",
            "Batch 290/455: Loss = 0.0918\n",
            "Batch 300/455: Loss = 0.0284\n",
            "Batch 310/455: Loss = 0.2488\n",
            "Batch 320/455: Loss = 0.0838\n",
            "Batch 330/455: Loss = 0.0252\n",
            "Batch 340/455: Loss = 0.0319\n",
            "Batch 350/455: Loss = 0.0330\n",
            "Batch 360/455: Loss = 0.1693\n",
            "Batch 370/455: Loss = 0.1420\n",
            "Batch 380/455: Loss = 0.0379\n",
            "Batch 390/455: Loss = 0.0365\n",
            "Batch 400/455: Loss = 0.0054\n",
            "Batch 410/455: Loss = 0.0534\n",
            "Batch 420/455: Loss = 0.0695\n",
            "Batch 430/455: Loss = 0.1042\n",
            "Batch 440/455: Loss = 0.0501\n",
            "Batch 450/455: Loss = 0.0497\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/25] completed.\n",
            "Train Loss: 0.0834 | Train Acc: 97.29%\n",
            "Val Loss: 0.3394 | Val Acc: 91.19%\n",
            "Precision: 91.71% | Recall: 90.79% | F1-score: 91.06%\n",
            "\n",
            "Epoch 10/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.0181\n",
            "Batch 10/455: Loss = 0.0638\n",
            "Batch 20/455: Loss = 0.0377\n",
            "Batch 30/455: Loss = 0.0262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40/455: Loss = 0.0214\n",
            "Batch 50/455: Loss = 0.0205\n",
            "Batch 60/455: Loss = 0.0169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70/455: Loss = 0.1174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80/455: Loss = 0.0453\n",
            "Batch 90/455: Loss = 0.0350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100/455: Loss = 0.0511\n",
            "Batch 110/455: Loss = 0.0101\n",
            "Batch 120/455: Loss = 0.0796\n",
            "Batch 130/455: Loss = 0.0264\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 140/455: Loss = 0.0302\n",
            "Batch 150/455: Loss = 0.0352\n",
            "Batch 160/455: Loss = 0.0230\n",
            "Batch 170/455: Loss = 0.0170\n",
            "Batch 180/455: Loss = 0.0730\n",
            "Batch 190/455: Loss = 0.0421\n",
            "Batch 200/455: Loss = 0.0559\n",
            "Batch 210/455: Loss = 0.0681\n",
            "Batch 220/455: Loss = 0.0498\n",
            "Batch 230/455: Loss = 0.0062\n",
            "Batch 240/455: Loss = 0.0166\n",
            "Batch 250/455: Loss = 0.0996\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 260/455: Loss = 0.2147\n",
            "Batch 270/455: Loss = 0.0616\n",
            "Batch 280/455: Loss = 0.0484\n",
            "Batch 290/455: Loss = 0.1696\n",
            "Batch 300/455: Loss = 0.0323\n",
            "Batch 310/455: Loss = 0.0508\n",
            "Batch 320/455: Loss = 0.0878\n",
            "Batch 330/455: Loss = 0.0295\n",
            "Batch 340/455: Loss = 0.0182\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 350/455: Loss = 0.2134\n",
            "Batch 360/455: Loss = 0.0558\n",
            "Batch 370/455: Loss = 0.0218\n",
            "Batch 380/455: Loss = 0.1265\n",
            "Batch 390/455: Loss = 0.0339\n",
            "Batch 400/455: Loss = 0.0785\n",
            "Batch 410/455: Loss = 0.0114\n",
            "Batch 420/455: Loss = 0.1893\n",
            "Batch 430/455: Loss = 0.0449\n",
            "Batch 440/455: Loss = 0.0902\n",
            "Batch 450/455: Loss = 0.0699\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/25] completed.\n",
            "Train Loss: 0.0711 | Train Acc: 97.76%\n",
            "Val Loss: 0.3164 | Val Acc: 91.19%\n",
            "Precision: 91.09% | Recall: 91.02% | F1-score: 91.02%\n",
            "Checkpoint saved at /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/DenseNet-121/checkpoint_epoch_10.pth!\n",
            "\n",
            "Epoch 11/25 started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/455: Loss = 0.0162\n",
            "Batch 10/455: Loss = 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20/455: Loss = 0.1844\n",
            "Batch 30/455: Loss = 0.0453\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/paper/paper 2273.jpg'>\n",
            "Batch 40/455: Loss = 0.1213\n",
            "Batch 50/455: Loss = 0.0214\n",
            "Batch 60/455: Loss = 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70/455: Loss = 0.0050\n",
            "Batch 80/455: Loss = 0.0391\n",
            "Batch 90/455: Loss = 0.0401\n",
            "Batch 100/455: Loss = 0.0084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110/455: Loss = 0.0177\n",
            "Batch 120/455: Loss = 0.0120\n",
            "Batch 130/455: Loss = 0.2357\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/e-waste/e-waste 1719.jpg'>\n",
            "Batch 140/455: Loss = 0.0500\n",
            "Skipping corrupted image: /content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg - cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/Waste Classification using Transfer Learning/Dataset/cardboard/cardboard 1075.jpg'>\n",
            "Batch 150/455: Loss = 0.0240\n",
            "Batch 160/455: Loss = 0.0069\n",
            "Batch 170/455: Loss = 0.0262\n",
            "Batch 180/455: Loss = 0.1273\n",
            "Batch 190/455: Loss = 0.1603\n",
            "Batch 200/455: Loss = 0.1707\n",
            "Batch 210/455: Loss = 0.1628\n",
            "Batch 220/455: Loss = 0.0884\n",
            "Batch 230/455: Loss = 0.0261\n",
            "Batch 240/455: Loss = 0.1745\n",
            "Batch 250/455: Loss = 0.0344\n",
            "Batch 260/455: Loss = 0.2142\n",
            "Batch 270/455: Loss = 0.0375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 280/455: Loss = 0.0594\n",
            "Batch 290/455: Loss = 0.0358\n",
            "Batch 300/455: Loss = 0.0951\n",
            "Batch 310/455: Loss = 0.0257\n",
            "Batch 320/455: Loss = 0.0136\n",
            "Batch 330/455: Loss = 0.0659\n",
            "Batch 340/455: Loss = 0.0264\n",
            "Batch 350/455: Loss = 0.0230\n",
            "Batch 360/455: Loss = 0.0254\n",
            "Batch 370/455: Loss = 0.0396\n",
            "Batch 380/455: Loss = 0.0841\n",
            "Batch 390/455: Loss = 0.0488\n",
            "Batch 400/455: Loss = 0.0170\n",
            "Batch 410/455: Loss = 0.0751\n",
            "Batch 420/455: Loss = 0.0430\n",
            "Batch 430/455: Loss = 0.0816\n",
            "Batch 440/455: Loss = 0.0062\n",
            "Batch 450/455: Loss = 0.0182\n",
            "Training phase completed. Starting validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/25] completed.\n",
            "Train Loss: 0.0687 | Train Acc: 97.76%\n",
            "Val Loss: 0.3264 | Val Acc: 91.62%\n",
            "Precision: 92.10% | Recall: 91.38% | F1-score: 91.59%\n",
            "Early stopping at epoch 11 due to no improvement in validation loss or accuracy.\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Define the folder path\n",
        "save_path = \"/content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models\"\n",
        "# os.makedirs(save_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "torch.save(model, os.path.join(save_path, \"Model_DenseNet121.pth\"))\n",
        "print(f\"Model saved successfully in {save_path}/Model_DenseNet121.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FO-gOGmfCVr",
        "outputId": "1201df3d-6c5b-461c-e2bd-96e6ad35e272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully in /content/drive/MyDrive/Waste Classification using Transfer Learning/Saved Models/Model_DenseNet121.pth\n"
          ]
        }
      ]
    }
  ]
}